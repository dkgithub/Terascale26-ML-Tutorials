{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: From Perceptrons to Deep Neural Networks\n",
    "\n",
    "## Welcome! üéì\n",
    "\n",
    "In this tutorial, you'll learn the fundamental challenges and solutions for training deep neural networks.\n",
    "\n",
    "**What you'll master:**\n",
    "- ‚ö° The vanishing gradient problem\n",
    "- üéØ Activation functions (sigmoid, tanh, ReLU)\n",
    "- üìä Detecting overfitting\n",
    "- üõ°Ô∏è Regularization techniques (L1, L2, Dropout)\n",
    "\n",
    "**Time commitment:** ~2 hours\n",
    "\n",
    "**Prerequisites:**\n",
    "- Basic Python and PyTorch\n",
    "- Understanding of neural networks\n",
    "- Completed Tutorial 1 (recommended)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Imports\n",
    "\n",
    "First, let's import all the necessary modules from our tutorial package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Tutorial modules\n",
    "from perceptron_to_DNN_tutorial.MultiLayerPerceptron import MultiLayerPerceptron\n",
    "from perceptron_to_DNN_tutorial.train import (\n",
    "    train_model_with_gradient_tracking,\n",
    "    train_model_with_validation_tracking\n",
    ")\n",
    "from perceptron_to_DNN_tutorial.utils import (\n",
    "    create_toy_dataset,\n",
    "    FeatureNormalizer\n",
    ")\n",
    "from perceptron_to_DNN_tutorial.plotting import (\n",
    "    plot_gradient_flow,\n",
    "    plot_layer_gradient_norms,\n",
    "    plot_regularization_comparison,\n",
    "    plot_results\n",
    ")\n",
    "from perceptron_to_DNN_tutorial.logger import get_logger\n",
    "\n",
    "# Initialize logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "Let's set up our experimental parameters. These control:\n",
    "- Network architecture (depth and width)\n",
    "- Data properties (polynomial complexity, noise)\n",
    "- Training hyperparameters\n",
    "\n",
    "**Note:** Feel free to modify these later to see how results change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CONFIGURATION\n",
    "# ========================================\n",
    "\n",
    "# Network Architecture\n",
    "# This is a DEEP network: 10 layers total (1 input ‚Üí 9 hidden ‚Üí 1 output)\n",
    "architecture_deep = [1, 128, 128, 128, 128, 128, 128, 128, 128, 128, 1]\n",
    "\n",
    "# Data Generation Parameters\n",
    "data_poly_order = 9      # High-order polynomial (complex function)\n",
    "n_train_samples = 200    # Training set size\n",
    "n_valid_samples = 200    # Validation set size  \n",
    "n_test_samples = 200     # Test set size\n",
    "noise_std = 2.5          # Noise level in data\n",
    "x_range = [0, 10]        # Input range\n",
    "\n",
    "# True polynomial coefficients (ground truth function)\n",
    "coeffs_true = [10.0, 0.5, -0.04, 0.015, -0.001, -0.0003, 0.000055, -0.000005, -1e-7, 2e-8]\n",
    "\n",
    "# Training Hyperparameters\n",
    "num_epochs = 10000       # Training iterations (for vanishing gradient demo)\n",
    "num_epochs_reg = 1000    # Fewer epochs for regularization experiments\n",
    "learning_rate = 0.005    # Step size for gradient descent\n",
    "\n",
    "# Activation functions to test\n",
    "activations_to_test = ['sigmoid', 'tanh', 'relu']\n",
    "\n",
    "# Regularization parameters\n",
    "lambda_l2 = 0.01         # L2 regularization strength\n",
    "dropout_rate = 0.3       # Dropout probability\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Polynomial order: {data_poly_order}\")\n",
    "print(f\"Training samples: {n_train_samples}\")\n",
    "print(f\"Validation samples: {n_valid_samples}\")\n",
    "print(f\"Test samples: {n_test_samples}\")\n",
    "print(f\"Network architecture: {' ‚Üí '.join(map(str, architecture_deep))}\")\n",
    "print(f\"Activations to test: {activations_to_test}\")\n",
    "print(f\"Training epochs (gradient demo): {num_epochs}\")\n",
    "print(f\"Training epochs (regularization): {num_epochs_reg}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: The Vanishing Gradient Problem ‚ö°\n",
    "\n",
    "## What is it?\n",
    "\n",
    "In deep networks, gradients can shrink exponentially as they backpropagate through layers:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial h_9} \\cdot \\frac{\\partial h_9}{\\partial h_8} \\cdot \\ldots \\cdot \\frac{\\partial h_2}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial W_1}$$\n",
    "\n",
    "With sigmoid/tanh, each derivative can be < 1, so the product **vanishes** (approaches 0).\n",
    "\n",
    "## Why does it matter?\n",
    "\n",
    "- Early layers get **tiny gradients** ‚Üí learn very slowly\n",
    "- Network becomes effectively **shallow**\n",
    "- Can't leverage the power of depth\n",
    "\n",
    "## What's the solution?\n",
    "\n",
    "**ReLU activation!** ReLU'(x) = 1 for x > 0, preventing gradient decay.\n",
    "\n",
    "Let's see this in action! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Generate Datasets üìä\n",
    "\n",
    "We'll create three separate datasets:\n",
    "- **Training**: Used to update weights\n",
    "- **Validation**: Used to monitor overfitting\n",
    "- **Test**: Used for final evaluation\n",
    "\n",
    "**Key principle:** Use different random seeds so datasets are truly independent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" PART 1: VANISHING GRADIENT PROBLEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[STEP 1.1] Generating train/validation/test datasets...\")\n",
    "\n",
    "# Generate training data\n",
    "x_train, y_train = create_toy_dataset(\n",
    "    coefficients=coeffs_true,\n",
    "    n_samples=n_train_samples,\n",
    "    x_range=x_range,\n",
    "    noise_std=noise_std,\n",
    "    random_seed=100  # Fixed seed for reproducibility\n",
    ")\n",
    "\n",
    "# Generate validation data (different seed!)\n",
    "x_valid, y_valid = create_toy_dataset(\n",
    "    coefficients=coeffs_true,\n",
    "    n_samples=n_valid_samples,\n",
    "    x_range=x_range,\n",
    "    noise_std=noise_std,\n",
    "    random_seed=200\n",
    ")\n",
    "\n",
    "# Generate test data (different seed!)\n",
    "x_test, y_test = create_toy_dataset(\n",
    "    coefficients=coeffs_true,\n",
    "    n_samples=n_test_samples,\n",
    "    x_range=x_range,\n",
    "    noise_std=noise_std,\n",
    "    random_seed=300\n",
    ")\n",
    "\n",
    "print(f\"‚úì Training: {len(x_train)} samples, y range [{y_train.min():.2f}, {y_train.max():.2f}]\")\n",
    "print(f\"‚úì Validation: {len(x_valid)} samples, y range [{y_valid.min():.2f}, {y_valid.max():.2f}]\")\n",
    "print(f\"‚úì Test: {len(x_test)} samples, y range [{y_test.min():.2f}, {y_test.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2: Normalize Features üîÑ\n",
    "\n",
    "Neural networks work best with normalized inputs. We'll normalize to the range [-1, 1].\n",
    "\n",
    "**Critical:** Use training statistics to normalize ALL datasets (train, val, test)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[STEP 1.2] Normalizing features...\")\n",
    "\n",
    "# Fit normalizer on training data only\n",
    "normalizer = FeatureNormalizer(method='symmetric')\n",
    "normalizer.fit(x_train)\n",
    "\n",
    "# Apply same normalization to all datasets\n",
    "x_t = normalizer.transform(x_train)\n",
    "y_t = y_train\n",
    "\n",
    "x_valid_norm = normalizer.transform(x_valid)\n",
    "x_test_norm = normalizer.transform(x_test)\n",
    "\n",
    "print(f\"‚úì Features normalized to [{x_t.min():.2f}, {x_t.max():.2f}] using training statistics\")\n",
    "print(f\"  (Validation and test use same normalization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.3: Train with Different Activations üèãÔ∏è\n",
    "\n",
    "Now we'll train **deep networks** (9 hidden layers) with three activation functions:\n",
    "\n",
    "1. **Sigmoid**: sigmoid(x) = 1/(1+e^(-x)), derivative ‚àà (0, 0.25]\n",
    "2. **Tanh**: tanh(x), derivative ‚àà (0, 1]\n",
    "3. **ReLU**: max(0, x), derivative = 1 for x > 0\n",
    "\n",
    "We'll track gradients at each layer to see the vanishing gradient problem in action!\n",
    "\n",
    "**This will take a few minutes** (training 30,000 epochs total). ‚òï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[STEP 1.3] Training deep networks with different activations...\")\n",
    "print(\"This demonstrates the vanishing gradient problem!\\n\")\n",
    "\n",
    "# Store training histories\n",
    "gradient_histories = {}\n",
    "\n",
    "for activation in activations_to_test:\n",
    "    print(f\"\\n--- Training with {activation.upper()} activation ---\")\n",
    "    \n",
    "    # Create model\n",
    "    model = MultiLayerPerceptron(\n",
    "        layer_sizes=architecture_deep,\n",
    "        activation=activation,\n",
    "        dropout_rate=0.0  # No dropout for gradient analysis\n",
    "    )\n",
    "    \n",
    "    # Train with gradient tracking and per-sample distributions\n",
    "    trained_model, history = train_model_with_gradient_tracking(\n",
    "        model=model,\n",
    "        x_train=x_t,\n",
    "        y_train=y_t,\n",
    "        x_valid=x_valid_norm,\n",
    "        y_valid=y_valid,\n",
    "        num_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        reg_type='none',  # No regularization\n",
    "        print_every=500,\n",
    "        verbose=True,\n",
    "        track_gradients=True,\n",
    "        track_per_sample_gradients=True  # For distribution plots\n",
    "    )\n",
    "    \n",
    "    # Store full history (includes per-sample gradients)\n",
    "    gradient_histories[activation] = history\n",
    "    \n",
    "    # Analyze final gradients\n",
    "    final_grads = history['gradient_norms'][-1]\n",
    "    layer_names = [name for name in final_grads.keys() if 'weight' in name]\n",
    "    \n",
    "    print(f\"\\nFinal gradient magnitudes ({activation}):\")\n",
    "    for layer_name in layer_names:\n",
    "        print(f\"  {layer_name}: {final_grads[layer_name]:.6e}\")\n",
    "    \n",
    "    # Check for vanishing\n",
    "    first_layer_grad = final_grads[layer_names[0]]\n",
    "    last_layer_grad = final_grads[layer_names[-1]]\n",
    "    if first_layer_grad > 0:\n",
    "        ratio = last_layer_grad / first_layer_grad\n",
    "        print(f\"\\nGradient ratio (last/first layer): {ratio:.6e}\")\n",
    "        if ratio < 0.01:\n",
    "            print(\"‚ö†Ô∏è  WARNING: VANISHING GRADIENT DETECTED!\")\n",
    "        else:\n",
    "            print(\"‚úì Gradients flowing reasonably through all layers\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training complete! Now let's visualize the results...\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4: Visualize Gradient Flow üìà\n",
    "\n",
    "This plot shows how gradient magnitudes change over training for each layer.\n",
    "\n",
    "**What to look for:**\n",
    "- **Sigmoid**: Gradients in early layers ‚Üí 0 (exponential decay)\n",
    "- **Tanh**: Moderate gradient decay\n",
    "- **ReLU**: Stable gradients across all layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[STEP 1.4] Visualizing gradient flow...\")\n",
    "\n",
    "example_model = MultiLayerPerceptron(architecture_deep, activation='relu')\n",
    "plot_gradient_flow(\n",
    "    gradient_histories, \n",
    "    example_model, \n",
    "    activations_to_test,\n",
    "    save_name='vanishing_gradient_demo.png'\n",
    ")\n",
    "\n",
    "print(\"\\nüí° Key Observation:\")\n",
    "print(\"   - Sigmoid: Gradients vanish in early layers (red warning box)\")\n",
    "print(\"   - Tanh: Moderate gradient flow\")\n",
    "print(\"   - ReLU: Healthy gradient flow (green box)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.5: Visualize Gradient Distributions üìä\n",
    "\n",
    "These plots show the **distribution** of signed gradient values across all training samples.\n",
    "\n",
    "We'll create **3 separate plots** for first, middle, and last epochs.\n",
    "\n",
    "**What to look for:**\n",
    "- **Healthy**: Wide distribution symmetric around zero\n",
    "- **Vanishing**: Distribution collapsed near zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[STEP 1.5] Visualizing gradient norm distributions across epochs...\")\n",
    "print(\"Creating 3 separate plots: first epoch, middle epoch, last epoch\\n\")\n",
    "\n",
    "plot_layer_gradient_norms(\n",
    "    gradient_histories,\n",
    "    example_model,\n",
    "    activations_to_test,\n",
    "    save_name='gradient_distributions'\n",
    ")\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   First epoch: All activations show reasonable gradients\")\n",
    "print(\"   Last epoch: Sigmoid collapsed to ~0, ReLU still healthy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.6: Visualize Best Model (ReLU) üèÜ\n",
    "\n",
    "Let's see how well the ReLU model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[STEP 1.6] Visualizing ReLU model fit...\")\n",
    "\n",
    "# Train a fresh ReLU model for visualization\n",
    "relu_model = MultiLayerPerceptron(architecture_deep, activation='relu', dropout_rate=0.0)\n",
    "relu_model, relu_history = train_model_with_gradient_tracking(\n",
    "    model=relu_model,\n",
    "    x_train=x_t,\n",
    "    y_train=y_t,\n",
    "    x_valid=x_valid_norm,\n",
    "    y_valid=y_valid,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    reg_type='none',\n",
    "    print_every=num_epochs + 1,  # Silent\n",
    "    track_gradients=False  # Faster\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "plot_results(\n",
    "    x_t, y_t,\n",
    "    relu_model,\n",
    "    relu_history,\n",
    "    coeffs_true,\n",
    "    data_poly_order,\n",
    "    model_name=\"Deep MLP with ReLU\",\n",
    "    normalizer=normalizer,\n",
    "    show_validation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Part 1 Summary\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "1. ‚ö†Ô∏è **Sigmoid and Tanh suffer from vanishing gradients** in deep networks\n",
    "   - Gradients shrink exponentially through layers\n",
    "   - Early layers barely learn\n",
    "\n",
    "2. ‚úÖ **ReLU solves the vanishing gradient problem**\n",
    "   - ReLU'(x) = 1 for x > 0 (no gradient decay)\n",
    "   - Gradients flow stably through all layers\n",
    "\n",
    "3. üìä **Gradient distributions reveal the problem**\n",
    "   - Sigmoid: Distribution collapses near zero\n",
    "   - ReLU: Distribution stays healthy\n",
    "\n",
    "**Key takeaway:** For deep networks, **always use ReLU** (or variants like Leaky ReLU, ELU)!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Detecting Overfitting üîç\n",
    "\n",
    "## What is overfitting?\n",
    "\n",
    "Overfitting occurs when a model **memorizes** training data instead of learning the underlying pattern.\n",
    "\n",
    "**Symptoms:**\n",
    "- Training loss ‚Üì‚Üì‚Üì (keeps decreasing)\n",
    "- Validation loss ‚Üë (starts increasing!)\n",
    "- Large gap between train and validation loss\n",
    "\n",
    "## How do we detect it?\n",
    "\n",
    "By tracking **both** training and validation loss:\n",
    "- **Good**: Both decrease together ‚Üí generalization\n",
    "- **Bad**: Train ‚Üì, Valid ‚Üë ‚Üí overfitting!\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Already Done! ‚úÖ\n",
    "\n",
    "We actually already demonstrated overfitting detection in Part 1:\n",
    "- Generated separate train/validation/test sets\n",
    "- Tracked both training and validation loss\n",
    "- Plots show the gap between train and validation curves\n",
    "\n",
    "**Look at the previous plots:**\n",
    "- The \"Train vs. Validation Loss\" panel shows both curves\n",
    "- The gap between them indicates overfitting level\n",
    "- Final gap is reported (e.g., \"Gap: +0.7 ‚ö†Ô∏è Overfitting\")\n",
    "\n",
    "With 200 samples and a 9th-order polynomial, we have enough data, so the gap should be small.\n",
    "\n",
    "Now let's see how **regularization** can reduce overfitting even further!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Regularization Techniques üõ°Ô∏è\n",
    "\n",
    "## What is regularization?\n",
    "\n",
    "Regularization prevents overfitting by:\n",
    "- Constraining model complexity\n",
    "- Keeping weights small\n",
    "- Adding controlled randomness\n",
    "\n",
    "## Three techniques:\n",
    "\n",
    "### 1. L1 Regularization (Lasso)\n",
    "Adds penalty: $\\lambda_1 \\sum_i |w_i|$\n",
    "- Promotes **sparse** weights (many ‚Üí 0)\n",
    "- Feature selection\n",
    "\n",
    "### 2. L2 Regularization (Ridge/Weight Decay)\n",
    "Adds penalty: $\\lambda_2 \\sum_i w_i^2$\n",
    "- Keeps weights **small**\n",
    "- Smooth solutions\n",
    "\n",
    "### 3. Dropout\n",
    "Randomly drops neurons during training:\n",
    "- Prevents co-adaptation\n",
    "- Ensemble-like behavior\n",
    "\n",
    "Let's compare them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Train WITHOUT Regularization (Baseline) üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" PART 3: REGULARIZATION TECHNIQUES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[STEP 3.1] Training WITHOUT regularization...\")\n",
    "print(\"This will show overfitting: train loss decreases but validation loss increases!\\n\")\n",
    "\n",
    "# Model without regularization\n",
    "model_no_reg = MultiLayerPerceptron(\n",
    "    layer_sizes=architecture_deep,\n",
    "    activation='relu',\n",
    "    dropout_rate=0.0\n",
    ")\n",
    "\n",
    "# Train\n",
    "_, history_no_reg = train_model_with_validation_tracking(\n",
    "    model=model_no_reg,\n",
    "    x_train=x_t,\n",
    "    y_train=y_t,\n",
    "    x_valid=x_valid_norm,\n",
    "    y_valid=y_valid,\n",
    "    num_epochs=num_epochs_reg,\n",
    "    learning_rate=learning_rate,\n",
    "    reg_type='none',\n",
    "    print_every=500,\n",
    "    track_gradients=False  # Speed up training\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Baseline model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Train WITH L2 Regularization üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[STEP 3.2] Training WITH L2 regularization...\")\n",
    "print(f\"L2 penalty: {lambda_l2}\\n\")\n",
    "\n",
    "# Model with L2\n",
    "model_l2 = MultiLayerPerceptron(\n",
    "    layer_sizes=architecture_deep,\n",
    "    activation='relu',\n",
    "    dropout_rate=0.0\n",
    ")\n",
    "\n",
    "# Train with L2\n",
    "_, history_l2 = train_model_with_validation_tracking(\n",
    "    model=model_l2,\n",
    "    x_train=x_t,\n",
    "    y_train=y_t,\n",
    "    x_valid=x_valid_norm,\n",
    "    y_valid=y_valid,\n",
    "    num_epochs=num_epochs_reg,\n",
    "    learning_rate=learning_rate,\n",
    "    reg_type='l2',\n",
    "    lambda_l2=lambda_l2,\n",
    "    print_every=500,\n",
    "    track_gradients=False\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì L2 regularized model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.3: Train WITH Dropout üé≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[STEP 3.3] Training WITH dropout...\")\n",
    "print(f\"Dropout rate: {dropout_rate}\\n\")\n",
    "\n",
    "# Model with dropout\n",
    "model_dropout = MultiLayerPerceptron(\n",
    "    layer_sizes=architecture_deep,\n",
    "    activation='relu',\n",
    "    dropout_rate=dropout_rate\n",
    ")\n",
    "\n",
    "# Train\n",
    "_, history_dropout = train_model_with_validation_tracking(\n",
    "    model=model_dropout,\n",
    "    x_train=x_t,\n",
    "    y_train=y_t,\n",
    "    x_valid=x_valid_norm,\n",
    "    y_valid=y_valid,\n",
    "    num_epochs=num_epochs_reg,\n",
    "    learning_rate=learning_rate,\n",
    "    reg_type='none',\n",
    "    print_every=500,\n",
    "    track_gradients=False\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Dropout model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.4: Compare Regularization Methods üìä\n",
    "\n",
    "Now let's visualize all three approaches side-by-side!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[STEP 3.4] Comparing regularization methods...\\n\")\n",
    "\n",
    "# Compare all methods\n",
    "plot_regularization_comparison(\n",
    "    histories=[history_no_reg, history_l2, history_dropout],\n",
    "    model_names=['No Regularization', 'L2 Regularization', 'Dropout (p=0.3)'],\n",
    "    save_name='regularization_comparison.png'\n",
    ")\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   - No regularization: Larger train-validation gap\")\n",
    "print(\"   - L2: Smaller gap, smoother training\")\n",
    "print(\"   - Dropout: Smaller gap, more stable\")\n",
    "print(\"\\n   Regularization keeps validation loss close to training loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.5: Visualize Dropout Model üé≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[STEP 3.5] Visualizing dropout model fit...\")\n",
    "\n",
    "plot_results(\n",
    "    x_t, y_t,\n",
    "    model_dropout,\n",
    "    history_dropout,\n",
    "    coeffs_true,\n",
    "    data_poly_order,\n",
    "    model_name=f\"Deep MLP with Dropout (p={dropout_rate})\",\n",
    "    normalizer=normalizer,\n",
    "    show_validation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Part 3 Summary\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "1. üìä **Without regularization**: Models can overfit\n",
    "   - Training loss keeps decreasing\n",
    "   - Validation loss plateaus or increases\n",
    "   - Large train-validation gap\n",
    "\n",
    "2. üéØ **L2 regularization**: Keeps weights small\n",
    "   - Adds penalty: $\\lambda_2 \\sum w_i^2$\n",
    "   - Reduces overfitting\n",
    "   - Smoother training curves\n",
    "\n",
    "3. üé≤ **Dropout**: Random neuron deactivation\n",
    "   - Prevents co-adaptation\n",
    "   - Acts like ensemble learning\n",
    "   - Very effective regularization\n",
    "\n",
    "**Key takeaway:** Always use regularization for better generalization!\n",
    "\n",
    "**Recommended approach:**\n",
    "- Start with L2 regularization (Œª ‚âà 0.01)\n",
    "- Add dropout (p ‚âà 0.3) for extra robustness\n",
    "- Monitor train-validation gap to tune hyperparameters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Tutorial Complete!\n",
    "\n",
    "## Congratulations! üéâ\n",
    "\n",
    "You've mastered the fundamentals of deep neural networks:\n",
    "\n",
    "### ‚úÖ What you learned:\n",
    "\n",
    "1. **Vanishing Gradients**\n",
    "   - Why sigmoid/tanh fail in deep networks\n",
    "   - How ReLU solves the problem\n",
    "   - Gradient flow visualization\n",
    "\n",
    "2. **Overfitting Detection**\n",
    "   - Train/validation/test splits\n",
    "   - Monitoring train-validation gap\n",
    "   - Identifying memorization vs learning\n",
    "\n",
    "3. **Regularization**\n",
    "   - L2 regularization (weight decay)\n",
    "   - Dropout (random deactivation)\n",
    "   - Preventing overfitting\n",
    "\n",
    "### üî¨ Experiments to Try:\n",
    "\n",
    "1. **Change network depth**: Try 5 layers vs 15 layers\n",
    "2. **Modify data complexity**: Change `data_poly_order` to 5 or 15\n",
    "3. **Tune regularization**: Try different Œª values (0.001, 0.1, 1.0)\n",
    "4. **Compare learning rates**: Test [0.001, 0.01, 0.1]\n",
    "5. **Add batch normalization**: Implement between layers\n",
    "\n",
    "### üìö Next Steps:\n",
    "\n",
    "- **Tutorial 3**: [From DNNs to Transformers](../docs/tutorials/tutorial-3.md)\n",
    "- Experiment with real datasets (MNIST, CIFAR-10)\n",
    "- Study advanced architectures (ResNets, Transformers)\n",
    "- Read the deep learning textbook\n",
    "\n",
    "### üí° Key Principles to Remember:\n",
    "\n",
    "1. **Always use ReLU** (or variants) for deep networks\n",
    "2. **Always split data** into train/val/test\n",
    "3. **Always monitor validation loss** to detect overfitting\n",
    "4. **Always use regularization** (L2 + dropout is a good default)\n",
    "5. **Always normalize inputs** before training\n",
    "\n",
    "---\n",
    "\n",
    "## üôã Questions?\n",
    "\n",
    "- Check the [FAQ](../docs/faq.md)\n",
    "- See [Troubleshooting](../docs/troubleshooting.md)\n",
    "- Open an issue on GitHub\n",
    "\n",
    "Happy learning! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
