{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Flow Matching - From SDEs to ODEs\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "- **Understand** the fundamental difference between SDE (DDPM) and ODE (Flow Matching) formulations\n",
    "- **Implement** conditional flow matching from scratch with probability paths and velocity fields\n",
    "- **Visualize** how data flows through probability space over time\n",
    "- **Compare** different ODE solvers (Euler vs RK45) and their trade-offs\n",
    "- **Analyze** when to use flow matching versus diffusion models\n",
    "\n",
    "## Key Conceptual Difference\n",
    "\n",
    "**DDPM (Tutorial 1)**: Stochastic Differential Equation\n",
    "$$dx = f(x,t)dt + g(t)dW \\quad \\text{[SDE with noise term]}$$\n",
    "- Stochastic (random) sampling process\n",
    "- Requires noise injection at each step\n",
    "- More robust but slower\n",
    "\n",
    "**Flow Matching (This Tutorial)**: Ordinary Differential Equation\n",
    "$$\\frac{dx}{dt} = v_\\theta(x,t) \\quad \\text{[ODE, deterministic]}$$\n",
    "- Deterministic path from noise to data\n",
    "- No noise needed during sampling\n",
    "- Can use sophisticated ODE solvers\n",
    "\n",
    "## Tutorial Structure\n",
    "\n",
    "This tutorial includes **TODOs** that you'll complete based on lecture material. You'll learn by implementing, not just running!\n",
    "\n",
    "**Expected time**: ~60-90 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Tutorial modules\n",
    "from flow_matching_tutorial.models import SimpleMLPDenoiser\n",
    "from flow_matching_tutorial.flow import ConditionalFlowMatching, check_implementation\n",
    "from flow_matching_tutorial.utils import (\n",
    "    create_toy_dataset,\n",
    "    ToyDataLoader,\n",
    "    set_seed,\n",
    "    count_parameters,\n",
    "    get_device,\n",
    ")\n",
    "from flow_matching_tutorial.visualization import (\n",
    "    visualize_probability_paths,\n",
    "    visualize_velocity_field,\n",
    "    visualize_samples,\n",
    "    plot_training_curves,\n",
    "    visualize_reverse_process_trajectory,\n",
    "    create_reverse_process_animation,\n",
    "    visualize_marginal_distributions,\n",
    "    compare_ode_solvers,\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Check Your Implementation\n",
    "\n",
    "Before we begin, let's verify that you've completed the required TODOs in `flow.py`.\n",
    "\n",
    "### What You Need to Implement:\n",
    "\n",
    "1. **TODO #1: Probability Paths** - How to interpolate between noise $x_0$ and data $x_1$\n",
    "   - Linear path: $x_t = (1-t) x_0 + t x_1$\n",
    "   - Variance preserving: $x_t = \\cos(\\frac{\\pi t}{2}) x_0 + \\sin(\\frac{\\pi t}{2}) x_1$\n",
    "\n",
    "2. **TODO #2: Velocity Field** - The time derivative of the path\n",
    "   - Linear: $u_t = x_1 - x_0$ (constant)\n",
    "   - Variance preserving: $u_t = -\\frac{\\pi}{2}\\sin(\\frac{\\pi t}{2})x_0 + \\frac{\\pi}{2}\\cos(\\frac{\\pi t}{2})x_1$ (time-varying)\n",
    "\n",
    "3. **TODO #3: Flow Matching Loss** - The training objective\n",
    "   $$\\mathcal{L} = \\mathbb{E}_{t, x_0, x_1} \\left[\\|v_\\theta(x_t, t) - u_t\\|^2\\right]$$\n",
    "\n",
    "If you haven't completed these yet, open `flow_matching_tutorial/flow.py` and implement them based on your lecture notes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if TODOs are implemented\n",
    "if not check_implementation():\n",
    "    print(\"\\nPlease complete the TODOs in flow.py before proceeding!\")\n",
    "    print(\"Refer to your lecture notes and the comments in the code.\")\n",
    "else:\n",
    "    print(\"\\nGreat! All TODOs completed. Let's continue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Configuration\n",
    "\n",
    "Let's set up our experiment parameters.\n",
    "\n",
    "### Key Parameters:\n",
    "- **dataset_type**: Which 2D distribution to model (\"moons\", \"circles\", \"swiss_roll\", \"two_gaussians\")\n",
    "- **path_type**: Probability path to use (\"linear\" or \"variance_preserving\")\n",
    "- **n_epochs**: Number of training iterations\n",
    "- **n_euler_steps**: Number of discretization steps for Euler ODE solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Get device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    # Data\n",
    "    \"dataset_type\": \"moons\",  # Try: \"moons\", \"circles\", \"swiss_roll\", \"two_gaussians\"\n",
    "    \"n_samples\": 10000,\n",
    "    \"noise\": 0.05,\n",
    "    \n",
    "    # Flow Matching - KEY CHOICE!\n",
    "    \"path_type\": \"variance_preserving\",  # Try: \"linear\", \"variance_preserving\"\n",
    "    \n",
    "    # Model Architecture\n",
    "    \"hidden_dim\": 128,\n",
    "    \"time_embed_dim\": 32,\n",
    "    \"n_layers\": 3,\n",
    "    \n",
    "    # Training\n",
    "    \"batch_size\": 256,\n",
    "    \"n_epochs\": 100,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \n",
    "    # Sampling\n",
    "    \"n_samples_to_generate\": 1000,\n",
    "    \"n_euler_steps\": 100,\n",
    "    \n",
    "    # Visualization\n",
    "    \"forward_timesteps\": [0.0, 0.25, 0.5, 0.75, 1.0],  # Which timesteps to visualize\n",
    "    \"n_reverse_plots\": 6,  # Number of reverse process plots\n",
    "}\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Create Dataset\n",
    "\n",
    "We'll use the same 2D toy datasets as Tutorial 1 for fair comparison.\n",
    "\n",
    "### Mathematical Setup:\n",
    "\n",
    "Our goal is to learn a transformation:\n",
    "$$p_0(x) \\xrightarrow{\\text{flow}} p_1(x)$$\n",
    "\n",
    "Where:\n",
    "- $p_0(x) = \\mathcal{N}(0, I)$ is Gaussian noise (easy to sample)\n",
    "- $p_1(x)$ is the complex data distribution (what we want to generate)\n",
    "\n",
    "### Why 2D Toy Data?\n",
    "- **Visualizable**: We can plot the entire distribution\n",
    "- **Fast**: Quick training for experimentation\n",
    "- **Pedagogical**: Easier to understand what the model is learning\n",
    "\n",
    "The same principles extend to high-dimensional data (images, audio, etc.)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "data = create_toy_dataset(\n",
    "    dataset_type=config[\"dataset_type\"],\n",
    "    n_samples=config[\"n_samples\"],\n",
    "    noise=config[\"noise\"],\n",
    ")\n",
    "\n",
    "print(f\"Created '{config['dataset_type']}' dataset\")\n",
    "print(f\"  Shape: {data.shape}\")\n",
    "print(f\"  Mean: [{data.mean(0)[0]:.3f}, {data.mean(0)[1]:.3f}]\")\n",
    "print(f\"  Std:  [{data.std(0)[0]:.3f}, {data.std(0)[1]:.3f}]\")\n",
    "\n",
    "# Visualize the target distribution\n",
    "plt.figure(figsize=(8, 8))\n",
    "data_np = data.cpu().numpy() if torch.is_tensor(data) else data\n",
    "plt.scatter(data_np[:, 0], data_np[:, 1], alpha=0.5, s=10, c='blue')\n",
    "plt.xlabel('$x_1$', fontsize=14)\n",
    "plt.ylabel('$x_2$', fontsize=14)\n",
    "plt.title(f'Target Distribution: {config[\"dataset_type\"]}', fontsize=16, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create data loader\n",
    "data_loader = ToyDataLoader(\n",
    "    data, batch_size=config[\"batch_size\"], shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"\\nData loader created with batch size {config['batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions:\n",
    "\n",
    "1. **Why do we use toy 2D datasets instead of real images for learning?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   Because we can visualize the entire probability distribution in 2D, making it easier to understand what the model is learning. The same principles apply to high-dimensional data!\n",
    "   </details>\n",
    "\n",
    "2. **What distribution will we start from when generating samples?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   We'll start from Gaussian noise $\\mathcal{N}(0, I)$ and flow it to the data distribution.\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Initialize Flow Matching\n",
    "\n",
    "Now let's create our Conditional Flow Matching instance.\n",
    "\n",
    "### Mathematical Framework:\n",
    "\n",
    "Flow matching learns a **probability path** $p_t(x)$ that interpolates between:\n",
    "- $p_0(x) = \\mathcal{N}(0, I)$ (noise)\n",
    "- $p_1(x) = p_{\\text{data}}(x)$ (data)\n",
    "\n",
    "The path is governed by a **velocity field** $v_\\theta(x, t)$:\n",
    "$$\\frac{dx}{dt} = v_\\theta(x, t)$$\n",
    "\n",
    "### Two Probability Paths:\n",
    "\n",
    "**1. Linear Path** (simple straight-line interpolation):\n",
    "$$x_t = (1-t)x_0 + tx_1$$\n",
    "- Constant velocity: $u_t = x_1 - x_0$\n",
    "- Fast to compute\n",
    "- Norm decreases in the middle\n",
    "\n",
    "**2. Variance Preserving** (geodesic on sphere):\n",
    "$$x_t = \\cos\\left(\\frac{\\pi t}{2}\\right)x_0 + \\sin\\left(\\frac{\\pi t}{2}\\right)x_1$$\n",
    "- Time-varying velocity: $u_t = -\\frac{\\pi}{2}\\sin\\left(\\frac{\\pi t}{2}\\right)x_0 + \\frac{\\pi}{2}\\cos\\left(\\frac{\\pi t}{2}\\right)x_1$\n",
    "- Preserves norm (when $\\|x_0\\| = \\|x_1\\|$)\n",
    "- Often better sample quality\n",
    "\n",
    "### Key Difference from DDPM:\n",
    "- **DDPM**: Uses a fixed noise schedule $\\beta_t$, adds noise stochastically\n",
    "- **Flow Matching**: Designs a probability path, deterministic ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize flow matching\n",
    "flow = ConditionalFlowMatching(\n",
    "    path_type=config[\"path_type\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(f\"Created Conditional Flow Matching\")\n",
    "print(f\"  Probability path: {config['path_type']}\")\n",
    "print(f\"  Process type: ODE (deterministic)\")\n",
    "print(f\"\\nCompare with Tutorial 1:\")\n",
    "print(f\"  DDPM: SDE (stochastic) - dx = f(x,t)dt + g(t)dW\")\n",
    "print(f\"  Flow: ODE (deterministic) - dx/dt = v(x,t)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Probability Paths\n",
    "\n",
    "Let's see how the two probability paths differ visually!\n",
    "\n",
    "**What to look for:**\n",
    "- Linear path: Straight line (chord)\n",
    "- Variance Preserving: Curved arc (geodesic)\n",
    "- Arrow lengths show velocity magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize both probability paths\n",
    "visualize_probability_paths(flow, save_path=\"outputs/probability_paths.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Forward Flow Process\n",
    "\n",
    "Let's see how the flow process transforms our data at different timesteps!\n",
    "\n",
    "**Forward Flow Process:**\n",
    "$$x_0 \\sim p_{\\text{data}}(x) \\xrightarrow{t \\in [0,1]} x_t \\sim p_t(x) \\xrightarrow{t=1} x_1 \\sim \\mathcal{N}(0,I)$$\n",
    "\n",
    "We'll visualize the data distribution at timesteps: $t \\in \\{0, 0.25, 0.5, 0.75, 1.0\\}$\n",
    "\n",
    "**What to observe:**\n",
    "- At $t=0$: Original data structure\n",
    "- At $t=0.5$: Partially \"flowed\" toward noise\n",
    "- At $t=1$: Should look like Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_forward_flow_process(\n",
    "    flow,\n",
    "    data,\n",
    "    timesteps,\n",
    "    n_samples=500,\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize how data flows toward noise at different timesteps.\n",
    "    \n",
    "    This shows the forward process: data → noise\n",
    "    (The reverse process during sampling goes: noise → data)\n",
    "    \"\"\"\n",
    "    n_steps = len(timesteps)\n",
    "    fig, axes = plt.subplots(1, n_steps, figsize=(5 * n_steps, 5))\n",
    "    \n",
    "    if n_steps == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Sample subset of data - handle both Tensor and numpy array\n",
    "    if torch.is_tensor(data):\n",
    "        x_1 = data[:n_samples].float().to(flow.device)\n",
    "    else:\n",
    "        x_1 = torch.from_numpy(data[:n_samples]).float().to(flow.device)\n",
    "    \n",
    "    for idx, (ax, t) in enumerate(zip(axes, timesteps)):\n",
    "        # Sample x_0 ~ N(0,I) (noise)\n",
    "        x_0 = torch.randn_like(x_1, device=flow.device)\n",
    "        \n",
    "        # Compute x_t along the probability path\n",
    "        t_tensor = torch.full((n_samples,), t, device=flow.device)\n",
    "        x_t = flow.sample_probability_path(x_0, x_1, t_tensor)\n",
    "        \n",
    "        # Convert to numpy for plotting\n",
    "        x_t_np = x_t.cpu().numpy()\n",
    "        \n",
    "        # Plot\n",
    "        ax.scatter(x_t_np[:, 0], x_t_np[:, 1], alpha=0.6, s=15, c='blue')\n",
    "        ax.set_xlim(-3, 3)\n",
    "        ax.set_ylim(-3, 3)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_title(f't = {t:.2f}', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        if idx == 0:\n",
    "            ax.set_ylabel('$x_2$', fontsize=12)\n",
    "        ax.set_xlabel('$x_1$', fontsize=12)\n",
    "        \n",
    "        # Add annotation\n",
    "        if t == 0.0:\n",
    "            ax.text(0.5, 0.95, 'Data', transform=ax.transAxes,\n",
    "                   fontsize=11, ha='center', va='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "        elif t == 1.0:\n",
    "            ax.text(0.5, 0.95, 'Noise', transform=ax.transAxes,\n",
    "                   fontsize=11, ha='center', va='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.7))\n",
    "        else:\n",
    "            ax.text(0.5, 0.95, 'Flowing...', transform=ax.transAxes,\n",
    "                   fontsize=11, ha='center', va='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "    \n",
    "    plt.suptitle(f'Forward Flow Process: Data → Noise ({config[\"path_type\"]} path)', \n",
    "                 fontsize=16, y=1.02, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved forward flow visualization to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Visualize forward flow process\n",
    "print(\"\\nVisualizing Forward Flow Process...\")\n",
    "print(f\"Timesteps: {config['forward_timesteps']}\")\n",
    "visualize_forward_flow_process(\n",
    "    flow,\n",
    "    data,\n",
    "    timesteps=config['forward_timesteps'],\n",
    "    save_path=\"outputs/forward_flow_process.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions:\n",
    "\n",
    "1. **What happens to the data structure as $t$ increases from 0 to 1?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   The structured data distribution gradually transforms into unstructured Gaussian noise. At t=0 we see the original pattern, and by t=1 it looks like random noise.\n",
    "   </details>\n",
    "\n",
    "2. **Why does the linear path's distribution \"shrink\" toward the origin at t=0.5?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   Linear interpolation averages the positions, so $x_t = 0.5x_0 + 0.5x_1$. When averaging vectors that point in different directions, the result has smaller magnitude (norm decreases).\n",
    "   </details>\n",
    "\n",
    "3. **How does variance preserving path differ visually at t=0.5?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   The variance preserving path maintains the \"spread\" better because it preserves the norm. The distribution doesn't shrink as much toward the origin.\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Create Model\n",
    "\n",
    "We use the **same architecture** as Tutorial 1!\n",
    "\n",
    "### Neural Network Architecture:\n",
    "\n",
    "The model $v_\\theta(x, t)$ predicts the velocity field:\n",
    "$$v_\\theta: \\mathbb{R}^d \\times [0,1] \\rightarrow \\mathbb{R}^d$$\n",
    "\n",
    "**Input:**\n",
    "- Position $x \\in \\mathbb{R}^2$ (current location in probability space)\n",
    "- Time $t \\in [0,1]$ (where we are along the path)\n",
    "\n",
    "**Output:**\n",
    "- Velocity $v \\in \\mathbb{R}^2$ (which direction to flow)\n",
    "\n",
    "### Key Insight:\n",
    "The only difference between DDPM and Flow Matching models:\n",
    "- **DDPM**: Model predicts **noise** $\\epsilon_\\theta(x, t)$\n",
    "- **Flow**: Model predicts **velocity** $v_\\theta(x, t)$\n",
    "\n",
    "The architecture is identical - only the interpretation changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = SimpleMLPDenoiser(\n",
    "    input_dim=2,\n",
    "    hidden_dim=config[\"hidden_dim\"],\n",
    "    time_embed_dim=config[\"time_embed_dim\"],\n",
    "    n_layers=config[\"n_layers\"],\n",
    ").to(device)\n",
    "\n",
    "n_params = count_parameters(model)\n",
    "print(f\"Created SimpleMLPDenoiser\")\n",
    "print(f\"  Architecture: 2D → {config['hidden_dim']} (×{config['n_layers']}) → 2D\")\n",
    "print(f\"  Time embedding: {config['time_embed_dim']} dimensions\")\n",
    "print(f\"  Total parameters: {n_params:,}\")\n",
    "print(f\"\\nNote: Same architecture as DDPM!\")\n",
    "print(f\"  DDPM output: Noise ε\")\n",
    "print(f\"  Flow output: Velocity v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions:\n",
    "\n",
    "1. **Why can we use the same model architecture for both DDPM and Flow Matching?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   Both tasks are fundamentally the same: given (x, t), predict a vector in the same space as x. Only the training target differs (noise vs velocity).\n",
    "   </details>\n",
    "\n",
    "2. **What does the time embedding do?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   It allows the model to condition its predictions on the current timestep t, so it can learn different behaviors at different times along the path.\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Train the Model\n",
    "\n",
    "Now we train the model to predict the velocity field!\n",
    "\n",
    "### Training Objective (Flow Matching Loss):\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{t \\sim \\mathcal{U}(0,1), x_0 \\sim \\mathcal{N}(0,I), x_1 \\sim p_{\\text{data}}} \\left[\\|v_\\theta(x_t, t) - u_t(x_t | x_0, x_1)\\|^2\\right]$$\n",
    "\n",
    "**Breaking it down:**\n",
    "1. Sample random time $t \\sim \\mathcal{U}(0,1)$\n",
    "2. Sample noise $x_0 \\sim \\mathcal{N}(0,I)$\n",
    "3. Sample data $x_1 \\sim p_{\\text{data}}$\n",
    "4. Compute point on path: $x_t = \\text{path}(x_0, x_1, t)$\n",
    "5. Compute target velocity: $u_t = \\frac{d}{dt}\\text{path}(x_0, x_1, t)$\n",
    "6. Predict velocity: $v_\\theta(x_t, t)$\n",
    "7. Minimize MSE: $\\|v_\\theta - u_t\\|^2$\n",
    "\n",
    "### Comparison with DDPM:\n",
    "\n",
    "| Aspect | DDPM | Flow Matching |\n",
    "|--------|------|---------------|\n",
    "| **Loss** | $\\mathbb{E}[\\|\\epsilon - \\epsilon_\\theta\\|^2]$ | $\\mathbb{E}[\\|v - v_\\theta\\|^2]$ |\n",
    "| **Target** | Noise $\\epsilon$ | Velocity $v$ |\n",
    "| **Sampling** | $t: T \\rightarrow 0$ (reverse) | $t: 0 \\rightarrow 1$ (forward) |\n",
    "| **Process** | Stochastic | Deterministic |\n",
    "\n",
    "Both are supervised regression - just different targets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "print(f\"Training Objective: Flow Matching Loss\")\n",
    "print(f\"   Target: Predict velocity v(x,t) at each point along the path\")\n",
    "print(f\"   Loss: MSE between predicted and true velocity\")\n",
    "print(f\"\\nOptimizer: Adam with lr={config['learning_rate']}\")\n",
    "print(f\"Training for {config['n_epochs']} epochs...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "losses = []\n",
    "model.train()\n",
    "\n",
    "for epoch in range(config['n_epochs']):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Progress bar for batches\n",
    "    pbar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{config['n_epochs']}\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # Compute flow matching loss (uses your TODO #3 implementation)\n",
    "        loss = flow.training_loss(model, batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record loss\n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # Average loss for epoch\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    # Print epoch summary every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{config['n_epochs']}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining complete! Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "plot_training_curves(losses, save_path=\"outputs/training_curve.png\")\n",
    "\n",
    "print(f\"\\nTraining Statistics:\")\n",
    "print(f\"   Initial loss: {losses[0]:.4f}\")\n",
    "print(f\"   Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"   Reduction: {(1 - losses[-1]/losses[0])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Learned Velocity Field\n",
    "\n",
    "Now let's see what velocity field the model learned!\n",
    "\n",
    "**The velocity field** $v_\\theta(x, t)$ tells us:\n",
    "- **Direction**: Which way to flow from position $x$\n",
    "- **Magnitude**: How fast to flow\n",
    "\n",
    "We'll visualize it at $t=0.5$ (midpoint of the path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize velocity field at t=0.5\n",
    "print(\"Visualizing learned velocity field at t=0.5...\")\n",
    "visualize_velocity_field(model, flow, t_value=0.5, save_path=\"outputs/velocity_field.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions:\n",
    "\n",
    "1. **What does the velocity field show?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   The arrows show the direction and speed of flow at each point in space. They point from noise (at t=1) toward data (at t=0) when we reverse the flow.\n",
    "   </details>\n",
    "\n",
    "2. **Why does the training loss decrease over time?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   The model is learning to better predict the target velocity at each point along the probability path. Lower loss = better velocity predictions.\n",
    "   </details>\n",
    "\n",
    "3. **How would you verify the model has learned the correct velocity field?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   By generating samples! If the learned velocity field is correct, following it from noise should produce samples that look like the training data.\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Generate Samples\n",
    "\n",
    "Time to generate new samples by solving the ODE!\n",
    "\n",
    "### The Sampling Process:\n",
    "\n",
    "We solve the **reverse ODE** from $t=0$ to $t=1$:\n",
    "$$\\frac{dx}{dt} = v_\\theta(x, t), \\quad x(0) \\sim \\mathcal{N}(0,I)$$\n",
    "\n",
    "**Two ODE Solvers:**\n",
    "\n",
    "**1. Euler Method** (simple, fixed step size):\n",
    "$$x_{t+\\Delta t} = x_t + v_\\theta(x_t, t) \\cdot \\Delta t$$\n",
    "- Simple to implement\n",
    "- Fast but less accurate\n",
    "- Requires many small steps\n",
    "\n",
    "**2. RK45 (Runge-Kutta)** (sophisticated, adaptive):\n",
    "- Automatically adjusts step size\n",
    "- More accurate with fewer evaluations\n",
    "- Recommended for production\n",
    "\n",
    "### Key Difference from DDPM:\n",
    "- **DDPM**: Iterative stochastic denoising (many steps, adds noise)\n",
    "- **Flow**: Solve ODE deterministically (can use adaptive solvers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Generating Samples using ODE Integration...\\n\")\n",
    "\n",
    "# Method 1: Euler solver\n",
    "print(f\"Method 1: Euler solver ({config['n_euler_steps']} steps)\")\n",
    "start_time = time.time()\n",
    "\n",
    "trajectory_euler = flow.sample(\n",
    "    model,\n",
    "    shape=(config[\"n_samples_to_generate\"], 2),\n",
    "    method=\"euler\",\n",
    "    n_steps=config[\"n_euler_steps\"],\n",
    "    return_trajectory=True,\n",
    ")\n",
    "\n",
    "euler_time = time.time() - start_time\n",
    "samples_euler = torch.from_numpy(trajectory_euler[-1])\n",
    "print(f\"  Sampling time: {euler_time:.2f}s\")\n",
    "print(f\"  NFE (function evaluations): {config['n_euler_steps']}\")\n",
    "print(f\"  Generated {len(samples_euler)} samples\")\n",
    "\n",
    "# Method 2: RK45 solver\n",
    "print(f\"\\nMethod 2: RK45 solver (adaptive steps)\")\n",
    "start_time = time.time()\n",
    "\n",
    "samples_rk45 = flow.sample(\n",
    "    model,\n",
    "    shape=(config[\"n_samples_to_generate\"], 2),\n",
    "    method=\"rk45\",\n",
    "    return_trajectory=False,\n",
    ")\n",
    "\n",
    "rk45_time = time.time() - start_time\n",
    "print(f\"  Sampling time: {rk45_time:.2f}s\")\n",
    "print(f\"  NFE: Adaptive (typically 20-50 evaluations)\")\n",
    "print(f\"  Generated {len(samples_rk45)} samples\")\n",
    "\n",
    "print(f\"\\nSpeed comparison: {euler_time / rk45_time:.2f}x (Euler/RK45)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Reverse Process Step-by-Step\n",
    "\n",
    "Let's see how samples evolve from noise to data!\n",
    "\n",
    "**Reverse Process**: $\\mathcal{N}(0,I) \\xrightarrow{\\text{ODE}} p_{\\text{data}}$\n",
    "\n",
    "We'll show the distribution at multiple timesteps as we integrate the ODE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reverse_process_steps(\n",
    "    trajectory,\n",
    "    n_plots=6,\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize the reverse process (noise → data) at multiple timesteps.\n",
    "    \"\"\"\n",
    "    n_steps = len(trajectory)\n",
    "    \n",
    "    # Select evenly spaced frames\n",
    "    indices = np.linspace(0, n_steps-1, n_plots, dtype=int)\n",
    "    \n",
    "    # Create grid of plots\n",
    "    n_cols = 3\n",
    "    n_rows = (n_plots + n_cols - 1) // n_cols\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 5*n_rows))\n",
    "    axes = axes.flatten() if n_plots > 1 else [axes]\n",
    "    \n",
    "    for idx, (ax, step_idx) in enumerate(zip(axes[:n_plots], indices)):\n",
    "        samples = trajectory[step_idx]\n",
    "        t_value = step_idx / (n_steps - 1)\n",
    "        \n",
    "        # Plot samples\n",
    "        ax.scatter(samples[:, 0], samples[:, 1], alpha=0.6, s=15, c='blue')\n",
    "        ax.set_xlim(-3, 3)\n",
    "        ax.set_ylim(-3, 3)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_title(f'Step {step_idx}/{n_steps-1}\\n(t = {t_value:.2f})', \n",
    "                    fontsize=13, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlabel('$x_1$', fontsize=11)\n",
    "        \n",
    "        if idx % n_cols == 0:\n",
    "            ax.set_ylabel('$x_2$', fontsize=11)\n",
    "        \n",
    "        # Add status annotation\n",
    "        if t_value < 0.1:\n",
    "            status = 'Noise'\n",
    "            color = 'lightyellow'\n",
    "        elif t_value > 0.9:\n",
    "            status = 'Data'\n",
    "            color = 'lightgreen'\n",
    "        else:\n",
    "            status = f'Flowing... {int(t_value*100)}%'\n",
    "            color = 'lightblue'\n",
    "        \n",
    "        ax.text(0.5, 0.95, status, transform=ax.transAxes,\n",
    "               fontsize=10, ha='center', va='top',\n",
    "               bbox=dict(boxstyle='round', facecolor=color, alpha=0.7))\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for ax in axes[n_plots:]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Reverse Process: Noise → Data (ODE Integration)', \n",
    "                 fontsize=16, y=1.0, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved reverse process steps to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Visualize reverse process\n",
    "print(f\"\\nVisualizing Reverse Process ({config['n_reverse_plots']} snapshots)...\")\n",
    "visualize_reverse_process_steps(\n",
    "    trajectory_euler,\n",
    "    n_plots=config['n_reverse_plots'],\n",
    "    save_path=\"outputs/reverse_process_steps.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Reverse Process Animation\n",
    "\n",
    "Let's create an animation showing the full transformation from noise to data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flow_animation(\n",
    "    trajectory,\n",
    "    save_path,\n",
    "    fps=20,\n",
    "    skip_frames=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Create animated GIF of the reverse flow process.\n",
    "    \"\"\"\n",
    "    print(f\"\\nCreating animation...\")\n",
    "    print(f\"   Total frames: {len(trajectory)}\")\n",
    "    print(f\"   Skip every {skip_frames} frames for smaller file size\")\n",
    "    print(f\"   FPS: {fps}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    # Use subset of frames for smaller file\n",
    "    frames_to_use = list(range(0, len(trajectory), skip_frames))\n",
    "    \n",
    "    def update(frame_idx):\n",
    "        ax.clear()\n",
    "        samples = trajectory[frames_to_use[frame_idx]]\n",
    "        t_value = frames_to_use[frame_idx] / (len(trajectory) - 1)\n",
    "        \n",
    "        ax.scatter(samples[:, 0], samples[:, 1], alpha=0.6, s=15, c='blue')\n",
    "        ax.set_xlim(-3, 3)\n",
    "        ax.set_ylim(-3, 3)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xlabel('$x_1$', fontsize=14)\n",
    "        ax.set_ylabel('$x_2$', fontsize=14)\n",
    "        ax.set_title(f'Flow Matching: Noise → Data\\n(t = {t_value:.2f})', \n",
    "                    fontsize=16, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Progress indicator\n",
    "        progress = int(t_value * 100)\n",
    "        ax.text(0.02, 0.98, f'Progress: {progress}%',\n",
    "               transform=ax.transAxes,\n",
    "               fontsize=12, va='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    anim = FuncAnimation(fig, update, frames=len(frames_to_use), interval=1000//fps)\n",
    "    \n",
    "    writer = PillowWriter(fps=fps)\n",
    "    anim.save(save_path, writer=writer)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"   Animation saved to {save_path}\")\n",
    "    print(f\"   Duration: {len(frames_to_use)/fps:.1f} seconds\")\n",
    "\n",
    "# Create animation\n",
    "create_flow_animation(\n",
    "    trajectory_euler,\n",
    "    save_path=\"outputs/flow_sampling.gif\",\n",
    "    fps=20,\n",
    "    skip_frames=2\n",
    ")\n",
    "\n",
    "print(\"\\nTip: Open outputs/flow_sampling.gif to see the full animation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare ODE Solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare quality of both solvers\n",
    "print(\"\\nComparing ODE Solvers...\")\n",
    "\n",
    "# Convert data to numpy if needed\n",
    "data_np = data.cpu().numpy() if torch.is_tensor(data) else data\n",
    "\n",
    "compare_ode_solvers(\n",
    "    samples_euler,\n",
    "    samples_rk45,\n",
    "    data_np[:config[\"n_samples_to_generate\"]],\n",
    "    nfe_euler=config[\"n_euler_steps\"],\n",
    "    nfe_rk45=30,  # Estimate\n",
    "    save_path=\"outputs/ode_solver_comparison.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions:\n",
    "\n",
    "1. **What's happening in the reverse process visualization?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   We start with random Gaussian noise (t=0) and integrate the learned velocity field forward in time. The ODE gradually transforms the noise into structured data that matches the training distribution.\n",
    "   </details>\n",
    "\n",
    "2. **Which ODE solver is better and why?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   RK45 is typically better because it adaptively adjusts the step size, achieving higher accuracy with fewer function evaluations. Euler is simpler but requires more steps for the same accuracy.\n",
    "   </details>\n",
    "\n",
    "3. **Why does the animation show smooth transitions?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   Because the ODE is deterministic and continuous. Unlike DDPM's discrete stochastic steps, flow matching follows a smooth continuous path through probability space.\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Compare Real vs Generated\n",
    "\n",
    "Let's see how well our flow matching model captured the data distribution!\n",
    "\n",
    "### Evaluation Metrics:\n",
    "\n",
    "We'll examine:\n",
    "1. **Visual similarity**: Do generated samples look like training data?\n",
    "2. **Marginal distributions**: Do individual dimensions match?\n",
    "3. **Wasserstein distance**: How far apart are the distributions?\n",
    "4. **Histogram overlap**: How much do the distributions overlap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RK45 samples for comparison (typically better quality)\n",
    "generated_samples = samples_rk45\n",
    "\n",
    "print(\"Comparing Real vs Generated Distributions...\\n\")\n",
    "\n",
    "# Convert data to numpy if needed\n",
    "data_np = data.cpu().numpy() if torch.is_tensor(data) else data\n",
    "\n",
    "# Visualize side-by-side comparison\n",
    "visualize_samples(\n",
    "    data_np[:config[\"n_samples_to_generate\"]],\n",
    "    generated_samples,\n",
    "    save_path=\"outputs/real_vs_generated.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative Analysis: 1D Marginal Distributions\n",
    "\n",
    "Let's look at each dimension separately with metrics!\n",
    "\n",
    "**Metrics:**\n",
    "- **Wasserstein Distance**: Measures how much \"work\" is needed to transform one distribution into another\n",
    "  - Lower is better (0 = perfect match)\n",
    "  - Typical good values: < 0.1\n",
    "\n",
    "- **Histogram Overlap**: Measures how much the histograms overlap\n",
    "  - Higher is better (1 = perfect overlap)\n",
    "  - Typical good values: > 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 1D marginals with metrics\n",
    "print(\"Analyzing marginal distributions with quantitative metrics...\")\n",
    "visualize_marginal_distributions(\n",
    "    data_np[:config[\"n_samples_to_generate\"]],\n",
    "    generated_samples,\n",
    "    save_path=\"outputs/marginal_distributions.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions:\n",
    "\n",
    "1. **How do you know if the model has learned the distribution well?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   \n",
    "   Multiple indicators:\n",
    "   - Visual similarity in scatter plots\n",
    "   - Low Wasserstein distance (< 0.1)\n",
    "   - High histogram overlap (> 0.8)\n",
    "   - Generated samples cover all modes of the data\n",
    "   </details>\n",
    "\n",
    "2. **What would it look like if the model failed?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   \n",
    "   Signs of failure:\n",
    "   - Generated samples cluster in one region (mode collapse)\n",
    "   - Samples don't match the shape of the data\n",
    "   - High Wasserstein distance (> 0.5)\n",
    "   - Low histogram overlap (< 0.5)\n",
    "   </details>\n",
    "\n",
    "3. **Why analyze marginal distributions separately?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   It helps identify if the model has trouble in specific dimensions. Even if the 2D plot looks good, one dimension might have systematic errors that marginal analysis reveals.\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Key Insights\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Probability Paths**: Different ways to interpolate between noise and data\n",
    "   - Linear: Straight line (simple, fast)\n",
    "   - Variance Preserving: Curved geodesic (often better quality)\n",
    "\n",
    "2. **Velocity Fields**: The derivatives that drive the flow\n",
    "   - $u_t = \\frac{d}{dt}x_t$ for each path type\n",
    "   - Model learns to predict these velocities\n",
    "\n",
    "3. **Flow Matching Loss**: Supervised learning of velocities\n",
    "   $$\\mathcal{L} = \\mathbb{E}[\\|v_\\theta(x_t, t) - u_t\\|^2]$$\n",
    "\n",
    "4. **ODE Solvers**: Different methods for numerical integration\n",
    "   - Euler: Simple, many steps needed\n",
    "   - RK45: Sophisticated, adaptive, fewer evaluations\n",
    "\n",
    "5. **SDE vs ODE**: Fundamental difference in generative modeling\n",
    "   - DDPM: Stochastic process with noise\n",
    "   - Flow: Deterministic ODE\n",
    "\n",
    "### Key Mathematical Insights:\n",
    "\n",
    "**Forward Process** (data → noise):\n",
    "$$x_t = \\text{path}(x_0, x_1, t), \\quad t: 0 \\rightarrow 1$$\n",
    "\n",
    "**Reverse Process** (noise → data):\n",
    "$$\\frac{dx}{dt} = v_\\theta(x, t), \\quad x(0) \\sim \\mathcal{N}(0,I)$$\n",
    "\n",
    "**Training**:\n",
    "$$\\text{Minimize: } \\mathbb{E}[\\|v_\\theta(x_t, t) - u_t\\|^2]$$\n",
    "\n",
    "### Comparison with Tutorial 1:\n",
    "\n",
    "| Aspect | DDPM (Tutorial 1) | Flow Matching (Tutorial 2) |\n",
    "|--------|-------------------|----------------------------|\n",
    "| **Process** | SDE (stochastic) | ODE (deterministic) |\n",
    "| **Forward** | Add noise with schedule | Follow probability path |\n",
    "| **Target** | Predict noise $\\epsilon$ | Predict velocity $v$ |\n",
    "| **Sampling** | Iterative denoising | ODE integration |\n",
    "| **Speed** | Fixed steps | Can use adaptive solvers |\n",
    "| **Model** | SimpleMLPDenoiser | **SAME** |\n",
    "| **Data** | Same datasets | **SAME** |\n",
    "\n",
    "### When to Use Each:\n",
    "\n",
    "**Use DDPM when:**\n",
    "- You want stochastic diversity\n",
    "- Problem naturally fits diffusion\n",
    "- More robust to errors is important\n",
    "\n",
    "**Use Flow Matching when:**\n",
    "- You want deterministic generation\n",
    "- You can design good probability paths\n",
    "- Faster sampling is needed (with good ODE solvers)\n",
    "- Problem has nice geometric structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiments to Try\n",
    "\n",
    "### Easy Experiments (10-15 minutes each):\n",
    "\n",
    "1. **Change the probability path**:\n",
    "   ```python\n",
    "   config[\"path_type\"] = \"linear\"  # Try this instead of variance preserving\n",
    "   ```\n",
    "   Compare training curves and sample quality. Which is better?\n",
    "\n",
    "2. **Try different datasets**:\n",
    "   ```python\n",
    "   config[\"dataset_type\"] = \"circles\"  # or \"swiss_roll\", \"two_gaussians\"\n",
    "   ```\n",
    "   Does one path type work better for certain datasets?\n",
    "\n",
    "3. **Vary the number of Euler steps**:\n",
    "   ```python\n",
    "   config[\"n_euler_steps\"] = 50  # or 25, 200\n",
    "   ```\n",
    "   How does this affect quality and speed?\n",
    "\n",
    "4. **Visualize velocity field at different times**:\n",
    "   ```python\n",
    "   for t in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "       visualize_velocity_field(model, flow, t_value=t)\n",
    "   ```\n",
    "   How does the velocity field change over time?\n",
    "\n",
    "### Medium Experiments (20-30 minutes each):\n",
    "\n",
    "5. **Compare training stability**:\n",
    "   - Train both linear and variance preserving on same data\n",
    "   - Compare loss curves\n",
    "   - Which converges faster?\n",
    "\n",
    "6. **Ablation study on model size**:\n",
    "   ```python\n",
    "   for hidden_dim in [64, 128, 256]:\n",
    "       # Train and compare sample quality\n",
    "   ```\n",
    "\n",
    "### Advanced Experiments (60+ minutes):\n",
    "\n",
    "7. **Design your own probability path**:\n",
    "   - Modify `flow.py` to add a custom path\n",
    "   - Try polynomial, exponential, or other interpolations\n",
    "   - Can you beat variance preserving?\n",
    "\n",
    "8. **Implement a different ODE solver**:\n",
    "   - Try RK4 or other Runge-Kutta methods\n",
    "   - Compare accuracy vs computational cost\n",
    "\n",
    "9. **Scale to higher dimensions**:\n",
    "   - Try 10D or 20D data\n",
    "   - How does performance change?\n",
    "   - Do different paths matter more in high dimensions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Reflection Questions\n",
    "\n",
    "### Conceptual Understanding:\n",
    "\n",
    "1. **What is the fundamental difference between DDPM and Flow Matching?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   DDPM uses a stochastic differential equation (SDE) with noise, while Flow Matching uses a deterministic ordinary differential equation (ODE). This makes Flow Matching deterministic and allows for sophisticated ODE solvers.\n",
    "   </details>\n",
    "\n",
    "2. **Why do we call it \"flow\" matching?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   Because we're learning a vector field (velocity) that causes probability mass to \"flow\" from the noise distribution to the data distribution, following continuous paths through probability space.\n",
    "   </details>\n",
    "\n",
    "3. **What role does the probability path play?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   The probability path defines how we interpolate between noise and data. It determines the geometry of the space we're moving through and affects both training stability and sample quality.\n",
    "   </details>\n",
    "\n",
    "### Practical Understanding:\n",
    "\n",
    "4. **When would you choose Flow Matching over DDPM?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   \n",
    "   Choose Flow Matching when:\n",
    "   - You need deterministic generation\n",
    "   - Faster sampling is important (can use adaptive ODE solvers)\n",
    "   - You can design good probability paths for your problem\n",
    "   - Your data has nice geometric structure\n",
    "   </details>\n",
    "\n",
    "5. **What are the trade-offs between Euler and RK45 solvers?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   \n",
    "   **Euler**: Simple, fast per step, but needs many steps for accuracy\n",
    "   **RK45**: Complex, adaptive steps, fewer evaluations needed, higher accuracy\n",
    "   \n",
    "   Use Euler for understanding/prototyping, RK45 for production.\n",
    "   </details>\n",
    "\n",
    "### Mathematical Understanding:\n",
    "\n",
    "6. **Why is the variance preserving path curved while linear is straight?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   Linear interpolation creates a straight line in Euclidean space (weighted average). Variance preserving uses trigonometric functions (cos/sin) which trace out a circular arc, creating a curved geodesic path that preserves the norm.\n",
    "   </details>\n",
    "\n",
    "7. **What does it mean to \"solve an ODE\" for sampling?**\n",
    "   <details>\n",
    "   <summary>Click for answer</summary>\n",
    "   We start from noise $x(0) \\sim \\mathcal{N}(0,I)$ and numerically integrate $\\frac{dx}{dt} = v_\\theta(x,t)$ forward in time from t=0 to t=1, following the learned velocity field to arrive at a data sample.\n",
    "   </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "### Further Learning:\n",
    "\n",
    "1. **Read the original papers**:\n",
    "   - Flow Matching: [\"Flow Matching for Generative Modeling\"](https://arxiv.org/abs/2210.02747)\n",
    "   - DDPM comparison: [\"Denoising Diffusion Probabilistic Models\"](https://arxiv.org/abs/2006.11239)\n",
    "   - Score-based models: [\"Score-Based Generative Modeling through SDEs\"](https://arxiv.org/abs/2011.13456)\n",
    "\n",
    "2. **Explore related topics**:\n",
    "   - Optimal Transport theory\n",
    "   - Riemannian geometry and manifolds\n",
    "   - Continuous Normalizing Flows (CNFs)\n",
    "   - Neural ODEs\n",
    "\n",
    "3. **Apply to real problems**:\n",
    "   - Scale to image generation (CIFAR-10, ImageNet)\n",
    "   - Try audio generation\n",
    "   - Experiment with molecular generation\n",
    "   - Apply to your own datasets\n",
    "\n",
    "### Questions or Issues?\n",
    "\n",
    "- Review your lecture notes\n",
    "- Check `flow_solutions.py` for reference implementations\n",
    "- Discuss with classmates or instructor\n",
    "- Experiment and debug - that's how you learn!\n",
    "\n",
    "### Course Progression:\n",
    "\n",
    "- Tutorial 1: DDPM (Stochastic Diffusion) - Complete\n",
    "- Tutorial 2: Flow Matching (Deterministic ODEs) - You are here\n",
    "- Tutorial 3: Advanced topics and comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations!\n",
    "\n",
    "You've completed Tutorial 2: Flow Matching!\n",
    "\n",
    "### You now understand:\n",
    "- How ODEs differ from SDEs for generative modeling\n",
    "- How to implement flow matching from scratch\n",
    "- How to design and compare probability paths\n",
    "- How to select appropriate ODE solvers\n",
    "- When to use flow matching vs diffusion models\n",
    "- How data flows through probability space\n",
    "- How to evaluate generative models quantitatively\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Flow Matching = Deterministic ODE-based generation**\n",
    "2. **Probability path design matters** for sample quality\n",
    "3. **Velocity fields are learned** just like noise in DDPM\n",
    "4. **ODE solvers can be adaptive** for efficiency\n",
    "5. **Same model architecture** works for both paradigms\n",
    "\n",
    "Keep experimenting and learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
